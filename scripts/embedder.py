#!/usr/bin/env python3
"""
Embedder ‚Äî –°–ª–æ–π 3: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ‚Üí ChromaDB.
–ß–∏—Ç–∞–µ—Ç content/chunks/*.json ‚Üí –ø–∏—à–µ—Ç –≤ embeddings/ (ChromaDB).

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–≤–∞ —Ä–µ–∂–∏–º–∞:
  - OpenAI (text-embedding-3-small) ‚Äî –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω—É–∂–µ–Ω API –∫–ª—é—á
  - Ollama (nomic-embed-text) ‚Äî –ª–æ–∫–∞–ª—å–Ω–æ, –±–µ—Å–ø–ª–∞—Ç–Ω–æ, —á—É—Ç—å —Ö—É–∂–µ
"""

import os
import json
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

load_dotenv(Path(__file__).parent / ".env")

REPO_PATH      = Path(os.getenv("REPO_PATH", Path(__file__).parent.parent))
CHUNKS_DIR     = REPO_PATH / "content" / "chunks"
EMBEDDINGS_DIR = REPO_PATH / "embeddings"

EMBED_MODE     = os.getenv("EMBED_MODE", "openai")  # "openai" –∏–ª–∏ "ollama"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
COLLECTION_NAME = "prompt-engineering-course"


def log(msg):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts}] {msg}")


def get_chroma_client():
    import chromadb
    return chromadb.PersistentClient(path=str(EMBEDDINGS_DIR))


def get_collection(client):
    return client.get_or_create_collection(
        name=COLLECTION_NAME,
        metadata={"hnsw:space": "cosine"}
    )


def embed_openai(texts: list) -> list:
    """–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ OpenAI API."""
    from openai import OpenAI
    client = OpenAI(api_key=OPENAI_API_KEY)
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=texts
    )
    return [item.embedding for item in response.data]


def embed_ollama(texts: list) -> list:
    """–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ Ollama (–ª–æ–∫–∞–ª—å–Ω–æ)."""
    import requests
    embeddings = []
    for text in texts:
        resp = requests.post(
            "http://localhost:11434/api/embeddings",
            json={"model": "nomic-embed-text", "prompt": text}
        )
        embeddings.append(resp.json()["embedding"])
    return embeddings


def embed_texts(texts: list) -> list:
    if EMBED_MODE == "ollama":
        log(f"  ü¶ô Ollama embeddings ({len(texts)} —Ç–µ–∫—Å—Ç–æ–≤)")
        return embed_ollama(texts)
    else:
        log(f"  ü§ñ OpenAI embeddings ({len(texts)} —Ç–µ–∫—Å—Ç–æ–≤)")
        return embed_openai(texts)


def embed():
    log("=" * 60)
    log(f"üî¢ –ó–∞–ø—É—Å–∫ —ç–º–±–µ–¥–¥–µ—Ä–∞ (—Ä–µ–∂–∏–º: {EMBED_MODE})")

    if EMBED_MODE == "openai" and not OPENAI_API_KEY:
        log("‚ùå OPENAI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω –≤ .env")
        log("   –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: —É—Å—Ç–∞–Ω–æ–≤–∏ Ollama –∏ –ø–æ—Å—Ç–∞–≤—å EMBED_MODE=ollama")
        return

    EMBEDDINGS_DIR.mkdir(exist_ok=True)
    client = get_chroma_client()
    collection = get_collection(client)

    # –£–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ chunk id
    existing = set(collection.get()["ids"])
    log(f"üìö –£–∂–µ –≤ –±–∞–∑–µ: {len(existing)} —á–∞–Ω–∫–æ–≤")

    chunk_files = list(CHUNKS_DIR.glob("*.json"))
    log(f"üìÇ –§–∞–π–ª–æ–≤ —á–∞–Ω–∫–æ–≤: {len(chunk_files)}")

    new_total = 0
    BATCH = 50  # –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

    all_chunks = []
    for chunk_file in chunk_files:
        chunks = json.loads(chunk_file.read_text())
        for chunk in chunks:
            chunk_id = f"{chunk['article_id']}__{chunk['chunk_index']}"
            if chunk_id not in existing:
                all_chunks.append((chunk_id, chunk))

    log(f"üÜï –ù–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {len(all_chunks)}")

    for i in range(0, len(all_chunks), BATCH):
        batch = all_chunks[i:i + BATCH]
        ids = [c[0] for c in batch]
        texts = [c[1]["text"] for c in batch]
        metadatas = [{
            "article_id": c[1]["article_id"],
            "article_title": c[1]["article_title"],
            "article_url": c[1]["article_url"],
            "module": c[1]["module"],
            "tags": ", ".join(c[1]["tags"]),
            "chunk_index": c[1]["chunk_index"],
        } for c in batch]

        try:
            embeddings = embed_texts(texts)
            collection.add(ids=ids, embeddings=embeddings, documents=texts, metadatas=metadatas)
            new_total += len(batch)
            log(f"  ‚úÖ –ü–∞–∫–µ—Ç {i // BATCH + 1}: +{len(batch)} —á–∞–Ω–∫–æ–≤")
        except Exception as e:
            log(f"  ‚ùå –û—à–∏–±–∫–∞ –ø–∞–∫–µ—Ç–∞ {i // BATCH + 1}: {e}")

    log(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ: +{new_total} —á–∞–Ω–∫–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ ChromaDB")
    log(f"üìä –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {collection.count()} —á–∞–Ω–∫–æ–≤")


if __name__ == "__main__":
    embed()
