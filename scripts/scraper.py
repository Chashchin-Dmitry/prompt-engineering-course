#!/usr/bin/env python3
"""
Medium Scraper â€” Prompt Engineering Course
Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ curl-cffi (Ğ¾Ğ±Ñ…Ğ¾Ğ´ Cloudflare) + pycookiecheat (ĞºÑƒĞºĞ¸ Ğ¸Ğ· Chrome).
ĞĞ¸ĞºĞ°ĞºĞ¾Ğ³Ğ¾ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ° â€” Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾, Ğ±ĞµĞ· Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸.
"""

import os
import re
import json
import time
import random
import subprocess
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

load_dotenv(Path(__file__).parent / ".env")

# â”€â”€ ĞŸÑƒÑ‚Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REPO_PATH    = Path(os.getenv("REPO_PATH", Path(__file__).parent.parent))
CONTENT_RAW  = REPO_PATH / "content" / "raw"
LOGS_DIR     = REPO_PATH / "logs"
KEYWORDS_FILE = Path(__file__).parent / "keywords.json"

# â”€â”€ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_ARTICLES    = int(os.getenv("MAX_ARTICLES_PER_RUN", 15))
QUERIES_PER_RUN = int(os.getenv("QUERIES_PER_RUN", 5))
MIN_DELAY       = float(os.getenv("MIN_DELAY", 1.5))
MAX_DELAY       = float(os.getenv("MAX_DELAY", 4))
AUTO_GIT_PUSH   = os.getenv("AUTO_GIT_PUSH", "true").lower() == "true"

# AI-Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ñ‹ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²
AI_TERMS_PATTERN = re.compile(
    r'\b(prompt\s+\w+|RAG|LLM|GPT-?\d*|Claude\s*\w*|Gemini\s*\w*|Llama\s*\d*|'
    r'chain.of.thought|few.shot|zero.shot|fine.tun\w+|embedding\w*|vector\s+\w+|'
    r'LangChain|LlamaIndex|CrewAI|AutoGPT|Cursor\s*AI|Copilot|Perplexity|'
    r'ReAct|tree.of.thoughts|self.consistency|function.calling|'
    r'Mistral|Codex|Grok|Qwen|GLM|Falcon|Mixtral|Gemma|'
    r'agent\s+\w+|agentic\s+\w+|multi.agent|autonomous\s+AI|'
    r'context.window|temperature\s+\w+|system.prompt|prompt.injection|'
    r'hallucination\s+\w+|grounding|guardrail\w*)\b',
    re.IGNORECASE
)


def log(msg):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{ts}] {msg}"
    print(line)
    LOGS_DIR.mkdir(exist_ok=True)
    with open(LOGS_DIR / "scraper.log", "a") as f:
        f.write(line + "\n")


def human_delay():
    time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))


# â”€â”€ Keywords DB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_keywords():
    if KEYWORDS_FILE.exists():
        return json.loads(KEYWORDS_FILE.read_text())
    return {"queue": [], "done": [], "discovered": [], "last_updated": ""}


def save_keywords(db):
    db["last_updated"] = datetime.now().strftime("%Y-%m-%d")
    KEYWORDS_FILE.write_text(json.dumps(db, ensure_ascii=False, indent=2))


def pick_queries(db, n=QUERIES_PER_RUN):
    available = [q for q in db["queue"] if q not in db["done"]]
    picks = available[:n]
    if len(picks) < n:
        db["done"] = []
        picks = db["queue"][:n]
    return picks


def add_discovered_keywords(db, text):
    found = set(AI_TERMS_PATTERN.findall(text))
    added = 0
    for term in found:
        t = term.strip().lower()
        if t not in db["queue"] and t not in db.get("discovered", []):
            db["queue"].append(t)
            db.setdefault("discovered", []).append(t)
            added += 1
    return added


# â”€â”€ HTTP Session â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def make_session():
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ ÑĞµÑÑĞ¸Ñ Ñ ĞºÑƒĞºĞ°Ğ¼Ğ¸ Ğ¸Ğ· Chrome Ğ¸ Chrome TLS-Ğ¾Ñ‚Ğ¿ĞµÑ‡Ğ°Ñ‚ĞºĞ¾Ğ¼."""
    from curl_cffi import requests as cf_requests
    from pycookiecheat import chrome_cookies

    cookies = chrome_cookies('https://medium.com')
    session = cf_requests.Session(impersonate='chrome131')
    session.cookies.update(cookies)
    session.headers.update({
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Referer': 'https://medium.com',
    })
    log(f"ğŸª ĞšÑƒĞºĞ¾Ğ² Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾: {len(cookies)}")
    return session


# â”€â”€ RSS Feed â†’ Article Links â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_article_links_rss(session, query, max_links=5):
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ½Ğ° ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ñ‡ĞµÑ€ĞµĞ· RSS â€” Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ JS-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°."""
    from bs4 import BeautifulSoup

    # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ query Ğ² Ñ‚ĞµĞ³ Medium
    tag = query.strip().lower().replace(' ', '-')
    url = f"https://medium.com/feed/tag/{tag}"
    log(f"ğŸ” RSS: '{query}' â†’ {url}")

    try:
        r = session.get(url, timeout=15)
        if r.status_code != 200:
            log(f"âš ï¸ RSS Ğ²ĞµÑ€Ğ½ÑƒĞ» {r.status_code} Ğ´Ğ»Ñ '{query}'")
            return []

        soup = BeautifulSoup(r.text, 'xml')
        items = soup.find_all('item')

        links = []
        for item in items[:max_links * 2]:
            link_el = item.find('link')
            guid_el = item.find('guid')
            url_art = (link_el.text if link_el else None) or (guid_el.text if guid_el else None)
            if url_art and 'medium.com' in url_art:
                links.append(url_art.split('?')[0])

        return links[:max_links]
    except Exception as e:
        log(f"âš ï¸ RSS Ğ¾ÑˆĞ¸Ğ±ĞºĞ° '{query}': {e}")
        return []


# â”€â”€ Article Fetcher â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_article(session, url):
    """Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¸ Ğ¿Ğ°Ñ€ÑĞ¸Ğ¼ ÑÑ‚Ğ°Ñ‚ÑŒÑ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ member-only."""
    from bs4 import BeautifulSoup

    try:
        r = session.get(url, timeout=20)
        if r.status_code != 200:
            log(f"âš ï¸ HTTP {r.status_code}: {url[:60]}")
            return None

        soup = BeautifulSoup(r.text, 'lxml')
        article_el = soup.find('article')
        if not article_el:
            return None

        # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ»Ğ¸ÑˆĞ½ĞµĞµ
        for tag in article_el.find_all(['button', 'nav', 'script', 'style', 'svg']):
            tag.decompose()

        content = article_el.get_text(separator='\n', strip=True)
        if len(content) < 300:
            return None

        title = soup.title.text.replace(' | Medium', '').strip() if soup.title else 'Unknown'
        title = re.sub(r' \| by .+$', '', title).strip()

        author_el = soup.find('a', attrs={'data-testid': 'authorName'}) or \
                    soup.find('a', rel='author')
        author = author_el.get_text(strip=True) if author_el else 'Unknown'

        time_el = soup.find('time')
        published = time_el['datetime'] if time_el and time_el.get('datetime') else None

        return {
            'title': title,
            'content': content,
            'author': author,
            'published': published,
            'url': url,
        }
    except Exception as e:
        log(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° {url[:60]}: {e}")
        return None


# â”€â”€ Save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def save_article(article):
    date_str = datetime.now().strftime("%Y-%m-%d")
    safe = re.sub(r'[^\w\s-]', '', article["title"])[:60].strip().replace(' ', '-')
    fname = f"{date_str}_{safe}"

    md = f"""# {article['title']}

**Source:** {article['url']}
**Author:** {article['author']}
**Published:** {article.get('published', 'unknown')}
**Scraped:** {date_str}

---

{article['content']}

---
*Auto-collected for Prompt Engineering Course*
"""
    CONTENT_RAW.mkdir(parents=True, exist_ok=True)
    (CONTENT_RAW / f"{fname}.md").write_text(md, encoding="utf-8")
    log(f"ğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾: {fname[:50]}")
    return fname


# â”€â”€ Git Push â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def git_push(count, new_kw):
    log("\nğŸ“¤ ĞŸÑƒÑˆÑƒ Ğ² GitHub...")
    try:
        os.chdir(REPO_PATH)
        subprocess.run(["git", "add", "content/", "scripts/keywords.json"], check=True)
        msg = (f"feat: scrape {count} articles, +{new_kw} new keywords "
               f"[{datetime.now().strftime('%Y-%m-%d')}]")
        result = subprocess.run(["git", "commit", "-m", msg], capture_output=True, text=True)
        if result.returncode == 0:
            subprocess.run(["git", "push"], check=True)
            log("âœ… Ğ—Ğ°Ğ¿ÑƒÑˆĞµĞ½Ğ¾!")
        else:
            log("â„¹ï¸ ĞĞµÑ‡ĞµĞ³Ğ¾ ĞºĞ¾Ğ¼Ğ¼Ğ¸Ñ‚Ğ¸Ñ‚ÑŒ")
    except Exception as e:
        log(f"âš ï¸ Git push: {e}")


# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def scrape():
    log("=" * 60)
    log("ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº ÑĞºÑ€Ğ°Ğ¿ĞµÑ€Ğ° (curl-cffi + pycookiecheat)")

    db = load_keywords()
    queries = pick_queries(db, QUERIES_PER_RUN)
    log(f"ğŸ“‹ Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹: {', '.join(queries)}")
    log(f"ğŸ“¦ Ğ›Ğ¸Ğ¼Ğ¸Ñ‚: {MAX_ARTICLES} ÑÑ‚Ğ°Ñ‚ĞµĞ¹ | ĞšĞ»ÑÑ‡ĞµĞ¹ Ğ² Ğ±Ğ°Ğ·Ğµ: {len(db['queue'])}")

    session = make_session()

    scraped_count = 0
    scraped_urls = set()
    new_keywords_total = 0

    for query in queries:
        if scraped_count >= MAX_ARTICLES:
            break

        db["done"].append(query)
        links = get_article_links_rss(session, query, max_links=4)
        log(f"  â†’ {len(links)} ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾")

        for url in links:
            if scraped_count >= MAX_ARTICLES:
                break
            if url in scraped_urls:
                continue
            scraped_urls.add(url)

            log(f"\nğŸ“– {url[:80]}")
            article = fetch_article(session, url)

            if article:
                save_article(article)
                nk = add_discovered_keywords(db, article["content"])
                new_keywords_total += nk
                scraped_count += 1
                log(f"âœ… [{scraped_count}/{MAX_ARTICLES}] '{article['title'][:50]}'")
            else:
                log("â­ï¸ ĞŸÑ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾ (Ğ¼Ğ°Ğ»Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° / Ğ¿ĞµĞ¹Ğ²Ğ¾Ğ»Ğ» Ğ±ĞµĞ· Ğ°ĞºĞºĞ°ÑƒĞ½Ñ‚Ğ°)")

            human_delay()

    save_keywords(db)

    log(f"\n{'='*60}")
    log(f"âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾: {scraped_count} ÑÑ‚Ğ°Ñ‚ĞµĞ¹ | +{new_keywords_total} Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ²")
    log(f"ğŸ“š Ğ‘Ğ°Ğ·Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²: {len(db['queue'])} Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²")

    if AUTO_GIT_PUSH and scraped_count > 0:
        git_push(scraped_count, new_keywords_total)


if __name__ == "__main__":
    scrape()
