#!/usr/bin/env python3
"""
Medium Scraper â€” Prompt Engineering Course
Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ½ĞµĞ¶Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼: Ğ¿Ğ°Ñ€ÑĞ¸Ğ¼ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ â†’ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹ â†’ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼ Ğ±Ğ°Ğ·Ñƒ.
"""

import os
import sys
import time
import random
import shutil
import subprocess
import json
import re
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

load_dotenv(Path(__file__).parent / ".env")

# â”€â”€ ĞŸÑƒÑ‚Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REPO_PATH       = Path(os.getenv("REPO_PATH", Path(__file__).parent.parent))
CONTENT_RAW     = REPO_PATH / "content" / "raw"
SCREENSHOTS_DIR = REPO_PATH / "content" / "screenshots"
KEYWORDS_FILE   = Path(__file__).parent / "keywords.json"
LOGS_DIR        = REPO_PATH / "logs"

CHROME_PROFILE_SRC = Path(os.getenv(
    "CHROME_PROFILE_PATH",
    os.path.expanduser("~/Library/Application Support/Google/Chrome/Default")
))
CHROME_PROFILE_TMP = REPO_PATH / "chrome-profile-copy"

# â”€â”€ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_ARTICLES    = int(os.getenv("MAX_ARTICLES_PER_RUN", 15))
QUERIES_PER_RUN = int(os.getenv("QUERIES_PER_RUN", 5))
MIN_DELAY       = float(os.getenv("MIN_DELAY", 2))
MAX_DELAY       = float(os.getenv("MAX_DELAY", 5))
AUTO_GIT_PUSH   = os.getenv("AUTO_GIT_PUSH", "true").lower() == "true"

# AI-Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ² Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ĞµĞ¹
AI_TERMS_PATTERN = re.compile(
    r'\b(prompt\s+\w+|RAG|LLM|GPT-?\d*|Claude\s*\w*|Gemini\s*\w*|Llama\s*\d*|'
    r'chain.of.thought|few.shot|zero.shot|fine.tun\w+|embedding\w*|vector\s+\w+|'
    r'LangChain|LlamaIndex|CrewAI|AutoGPT|Cursor\s*AI|Copilot|Perplexity|'
    r'ReAct|tree.of.thoughts|self.consistency|function.calling|'
    r'Mistral|Codex|Grok|Qwen|GLM|Falcon|Mixtral|Gemma|'
    r'agent\s+\w+|agentic\s+\w+|multi.agent|autonomous\s+AI|'
    r'context.window|temperature\s+\w+|system.prompt|prompt.injection|'
    r'hallucination\s+\w+|grounding|guardrail\w*)\b',
    re.IGNORECASE
)


def log(msg):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{ts}] {msg}"
    print(line)
    LOGS_DIR.mkdir(exist_ok=True)
    with open(LOGS_DIR / "scraper.log", "a") as f:
        f.write(line + "\n")


def human_delay(mn=None, mx=None):
    time.sleep(random.uniform(mn or MIN_DELAY, mx or MAX_DELAY))


def human_scroll(page, steps=5):
    for _ in range(steps):
        page.mouse.wheel(0, random.randint(200, 500))
        time.sleep(random.uniform(0.3, 0.8))


# â”€â”€ Keywords DB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_keywords():
    if KEYWORDS_FILE.exists():
        return json.loads(KEYWORDS_FILE.read_text())
    return {"queue": [], "done": [], "discovered": [], "last_updated": ""}


def save_keywords(db):
    db["last_updated"] = datetime.now().strftime("%Y-%m-%d")
    KEYWORDS_FILE.write_text(json.dumps(db, ensure_ascii=False, indent=2))


def pick_queries(db, n=QUERIES_PER_RUN):
    """Ğ‘ĞµÑ€Ñ‘Ğ¼ n Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸, Ğ½Ğµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑĞµĞ¼ ÑƒĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ."""
    available = [q for q in db["queue"] if q not in db["done"]]
    picks = available[:n]
    # Ğ•ÑĞ»Ğ¸ Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ ĞºĞ¾Ğ½Ñ‡Ğ°ĞµÑ‚ÑÑ â€” Ğ±ĞµÑ€Ñ‘Ğ¼ ÑĞ½Ğ¾Ğ²Ğ° Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° (Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½Ğ°)
    if len(picks) < n:
        db["done"] = []
        picks = db["queue"][:n]
    return picks


def add_discovered_keywords(db, text):
    """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ğµ AI-Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ."""
    found = set(AI_TERMS_PATTERN.findall(text))
    new_terms = []
    for term in found:
        term = term.strip().lower()
        if (term not in [q.lower() for q in db["queue"]] and
                term not in [d.lower() for d in db["discovered"]] and
                len(term) > 4):
            db["discovered"].append(term)
            db["queue"].append(term)
            new_terms.append(term)
    if new_terms:
        log(f"  ğŸ” ĞĞ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹: {', '.join(new_terms[:5])}{'...' if len(new_terms) > 5 else ''}")
    return len(new_terms)


# â”€â”€ Chrome â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def copy_chrome_profile():
    if CHROME_PROFILE_TMP.exists():
        shutil.rmtree(CHROME_PROFILE_TMP)
    CHROME_PROFILE_TMP.mkdir(parents=True, exist_ok=True)
    log("ğŸ“‹ ĞšĞ¾Ğ¿Ğ¸Ñ€ÑƒÑ Chrome Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ...")
    subprocess.run([
        "rsync", "-a", "--quiet",
        "--exclude=SingletonLock",
        "--exclude=SingletonCookie",
        "--exclude=GPUCache",
        "--exclude=*.log",
        f"{CHROME_PROFILE_SRC}/",
        f"{CHROME_PROFILE_TMP}/"
    ], capture_output=True)
    log("âœ… ĞŸÑ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ ÑĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½")


def cleanup_chrome_profile():
    if CHROME_PROFILE_TMP.exists():
        shutil.rmtree(CHROME_PROFILE_TMP)
    log("ğŸ§¹ Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ ÑƒĞ´Ğ°Ğ»Ñ‘Ğ½")


# â”€â”€ Scraping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_article_links(page, query, max_links=5):
    url = f"https://medium.com/search?q={query.replace(' ', '%20')}&source=search_post---------0"
    log(f"ğŸ” Ğ˜Ñ‰Ñƒ: '{query}'")
    try:
        page.goto(url, timeout=20000)
        human_delay(3, 6)
        human_scroll(page, steps=4)

        links = page.evaluate("""() => {
            const links = new Set();
            document.querySelectorAll('a[href]').forEach(a => {
                const h = a.href;
                if (h && h.includes('medium.com') &&
                    (h.includes('/p/') || (h.match(/medium\\.com\\/@[^/]+\\/[^/?]+/))) &&
                    !h.includes('/search') && !h.includes('/tag/') &&
                    !h.includes('/m/signin') && !h.includes('?source=follow')) {
                    links.add(h.split('?')[0]);
                }
            });
            return Array.from(links).slice(0, 10);
        }""")
        return links[:max_links]
    except Exception as e:
        log(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° '{query}': {e}")
        return []


def extract_article(page):
    try:
        page.wait_for_selector("article", timeout=12000)
        human_scroll(page, steps=8)
        human_delay(2, 4)

        data = page.evaluate("""() => {
            const article = document.querySelector('article');
            if (!article) return null;
            const unwanted = article.querySelectorAll('button, nav, [role="navigation"], script, style');
            unwanted.forEach(el => el.remove());
            const title = document.title.replace(/ \\| Medium$/, '').replace(/ \\| by .+$/, '').trim();
            const authorEl = document.querySelector('a[data-testid="authorName"], [rel="author"]');
            const dateEl = document.querySelector('time');
            return {
                title: title,
                content: article.innerText.trim(),
                author: authorEl ? authorEl.innerText.trim() : 'Unknown',
                published: dateEl ? dateEl.getAttribute('datetime') : null
            };
        }""")
        if data:
            data["url"] = page.url
        return data
    except Exception as e:
        log(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ: {e}")
        return None


def save_article(article, screenshot=None):
    date_str = datetime.now().strftime("%Y-%m-%d")
    safe = re.sub(r'[^\w\s-]', '', article["title"])[:60].strip().replace(' ', '-')
    fname = f"{date_str}_{safe}"

    md = f"""# {article['title']}

**Source:** {article['url']}
**Author:** {article['author']}
**Published:** {article.get('published', 'unknown')}
**Scraped:** {date_str}

---

{article['content']}

---
*Auto-collected for Prompt Engineering Course*
"""
    CONTENT_RAW.mkdir(parents=True, exist_ok=True)
    (CONTENT_RAW / f"{fname}.md").write_text(md, encoding="utf-8")

    if screenshot:
        SCREENSHOTS_DIR.mkdir(parents=True, exist_ok=True)
        (SCREENSHOTS_DIR / f"{fname}.png").write_bytes(screenshot)

    log(f"ğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾: {fname[:50]}")
    return fname


# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def scrape():
    from playwright.sync_api import sync_playwright
    from playwright_stealth import stealth_sync

    log("=" * 60)
    log(f"ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº ÑĞºÑ€Ğ°Ğ¿ĞµÑ€Ğ°")

    db = load_keywords()
    queries = pick_queries(db, QUERIES_PER_RUN)
    log(f"ğŸ“‹ Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹: {', '.join(queries)}")
    log(f"ğŸ“¦ Ğ›Ğ¸Ğ¼Ğ¸Ñ‚: {MAX_ARTICLES} ÑÑ‚Ğ°Ñ‚ĞµĞ¹ | ĞšĞ»ÑÑ‡ĞµĞ¹ Ğ² Ğ±Ğ°Ğ·Ğµ: {len(db['queue'])}")

    copy_chrome_profile()

    scraped_count = 0
    scraped_urls = set()
    new_keywords_total = 0

    try:
        with sync_playwright() as p:
            ctx = p.chromium.launch_persistent_context(
                user_data_dir=str(CHROME_PROFILE_TMP),
                channel="chrome",
                headless=False,
                args=[
                    "--disable-blink-features=AutomationControlled",
                    "--no-first-run",
                    "--no-default-browser-check",
                    "--disable-extensions",
                ],
                slow_mo=random.randint(60, 140),
            )

            page = ctx.new_page()
            stealth_sync(page)
            page.set_extra_http_headers({"Accept-Language": "en-US,en;q=0.9"})

            for query in queries:
                if scraped_count >= MAX_ARTICLES:
                    break

                db["done"].append(query)
                links = get_article_links(page, query, max_links=4)
                log(f"  â†’ {len(links)} ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾")

                for url in links:
                    if scraped_count >= MAX_ARTICLES:
                        break
                    if url in scraped_urls:
                        continue
                    scraped_urls.add(url)

                    try:
                        log(f"\nğŸ“– {url[:80]}")
                        page.goto(url, timeout=20000)
                        human_delay(4, 7)

                        screenshot = page.screenshot(full_page=True)
                        article = extract_article(page)

                        if article and len(article.get("content", "")) > 500:
                            save_article(article, screenshot)
                            # Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ñ‹
                            nk = add_discovered_keywords(db, article["content"])
                            new_keywords_total += nk
                            scraped_count += 1
                            log(f"âœ… [{scraped_count}/{MAX_ARTICLES}] '{article['title'][:50]}'")
                        else:
                            log("â­ï¸ ĞŸÑ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾ (Ğ¼Ğ°Ğ»Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ»Ğ¸ Ğ¿ĞµĞ¹Ğ²Ğ¾Ğ»Ğ»)")

                        human_delay(3, 6)

                    except Exception as e:
                        log(f"âŒ {url}: {e}")
                        continue

            ctx.close()

    except Exception as e:
        log(f"âŒ ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}")
    finally:
        cleanup_chrome_profile()

    save_keywords(db)

    log(f"\n{'='*60}")
    log(f"âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾: {scraped_count} ÑÑ‚Ğ°Ñ‚ĞµĞ¹ | +{new_keywords_total} Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ²")
    log(f"ğŸ“š Ğ‘Ğ°Ğ·Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²: {len(db['queue'])} Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²")

    if AUTO_GIT_PUSH and scraped_count > 0:
        git_push(scraped_count, new_keywords_total)


def git_push(count, new_kw):
    log("\nğŸ“¤ ĞŸÑƒÑˆÑƒ Ğ² GitHub...")
    try:
        os.chdir(REPO_PATH)
        subprocess.run(["git", "add", "content/", "scripts/keywords.json"], check=True)
        msg = (f"feat: scrape {count} articles, +{new_kw} new keywords "
               f"[{datetime.now().strftime('%Y-%m-%d')}]")
        result = subprocess.run(["git", "commit", "-m", msg], capture_output=True, text=True)
        if result.returncode == 0:
            subprocess.run(["git", "push"], check=True)
            log("âœ… Ğ—Ğ°Ğ¿ÑƒÑˆĞµĞ½Ğ¾!")
        else:
            log("â„¹ï¸ ĞĞµÑ‡ĞµĞ³Ğ¾ ĞºĞ¾Ğ¼Ğ¼Ğ¸Ñ‚Ğ¸Ñ‚ÑŒ")
    except Exception as e:
        log(f"âš ï¸ Git push: {e}")


if __name__ == "__main__":
    scrape()
