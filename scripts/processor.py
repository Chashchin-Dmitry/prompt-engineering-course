#!/usr/bin/env python3
"""
Processor ‚Äî –°–ª–æ–π 2: –æ—á–∏—Å—Ç–∫–∞, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ, —á–∞–Ω–∫–∏–Ω–≥, –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è.
–ß–∏—Ç–∞–µ—Ç content/raw/*.md ‚Üí –ø–∏—à–µ—Ç content/processed/*.json + content/chunks/*.json
"""

import os
import json
import re
import hashlib
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

load_dotenv(Path(__file__).parent / ".env")

REPO_PATH      = Path(os.getenv("REPO_PATH", Path(__file__).parent.parent))
RAW_DIR        = REPO_PATH / "content" / "raw"
PROCESSED_DIR  = REPO_PATH / "content" / "processed"
CHUNKS_DIR     = REPO_PATH / "content" / "chunks"
INDEX_FILE     = REPO_PATH / "content" / "index.json"

CHUNK_SIZE     = 500    # —Ç–æ–∫–µ–Ω–æ–≤ (~2000 —Å–∏–º–≤–æ–ª–æ–≤)
CHUNK_OVERLAP  = 50     # overlap –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏

# –ú–∞–ø–ø–∏–Ω–≥ —Ç–µ–≥–æ–≤ ‚Üí –º–æ–¥—É–ª–∏ –∫—É—Ä—Å–∞
TAG_MODULE_MAP = {
    "foundation": "01-foundations",
    "basic": "01-foundations",
    "introduction": "01-foundations",
    "chain-of-thought": "02-core-techniques",
    "few-shot": "02-core-techniques",
    "zero-shot": "02-core-techniques",
    "role-prompting": "02-core-techniques",
    "system-prompt": "02-core-techniques",
    "RAG": "03-advanced",
    "agent": "03-advanced",
    "multi-agent": "03-advanced",
    "fine-tuning": "03-advanced",
    "embedding": "03-advanced",
    "ChatGPT": "04-tools",
    "Claude": "04-tools",
    "Gemini": "04-tools",
    "Cursor": "04-tools",
    "LangChain": "04-tools",
    "Copilot": "04-tools",
    "developer": "05-use-cases",
    "marketing": "05-use-cases",
    "product": "05-use-cases",
    "best-practice": "06-best-practices",
    "checklist": "06-best-practices",
    "template": "06-best-practices",
}


def log(msg):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{ts}] {msg}")


def content_hash(text):
    return hashlib.sha256(text.encode()).hexdigest()[:16]


def parse_raw_md(path: Path) -> dict:
    """–ü–∞—Ä—Å–∏–º raw markdown ‚Üí —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π dict."""
    text = path.read_text(encoding="utf-8")
    lines = text.split("\n")

    meta = {}
    content_lines = []
    in_content = False

    for line in lines:
        if line.startswith("# ") and "title" not in meta:
            meta["title"] = line[2:].strip()
        elif line.startswith("**Source:**"):
            meta["url"] = re.search(r'https?://\S+', line)
            meta["url"] = meta["url"].group() if meta["url"] else ""
            meta["url"] = meta["url"].rstrip(")")
        elif line.startswith("**Author:**"):
            meta["author"] = line.replace("**Author:**", "").strip()
        elif line.startswith("**Published:**"):
            meta["published"] = line.replace("**Published:**", "").strip()
        elif line.startswith("**Scraped:**"):
            meta["scraped"] = line.replace("**Scraped:**", "").strip()
        elif line == "---" and not in_content:
            in_content = True
        elif in_content and not line.startswith("*Auto-collected"):
            content_lines.append(line)

    content = "\n".join(content_lines).strip()
    return {**meta, "content": content, "source_file": path.name}


def clean_text(text: str) -> str:
    """–ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞."""
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r' {2,}', ' ', text)
    # –£–±–∏—Ä–∞–µ–º —Ç–∏–ø–∏—á–Ω—ã–π Medium-–º—É—Å–æ—Ä
    text = re.sub(r'Sign up\s+Sign in.*?$', '', text, flags=re.MULTILINE)
    text = re.sub(r'Follow\s+\d+ Followers.*?$', '', text, flags=re.MULTILINE)
    return text.strip()


def extract_tags(text: str, title: str) -> list:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é."""
    combined = (title + " " + text[:3000]).lower()
    tags = []

    keyword_tags = {
        "prompt engineering": "prompt-engineering",
        "chain of thought": "chain-of-thought",
        "few-shot": "few-shot",
        "zero-shot": "zero-shot",
        "system prompt": "system-prompt",
        "role prompt": "role-prompting",
        "rag": "RAG",
        "retrieval": "RAG",
        "agent": "agent",
        "langchain": "LangChain",
        "chatgpt": "ChatGPT",
        "claude": "Claude",
        "gemini": "Gemini",
        "cursor": "Cursor",
        "copilot": "Copilot",
        "fine-tun": "fine-tuning",
        "embedding": "embedding",
        "developer": "developer",
        "best practice": "best-practice",
        "template": "template",
        "checklist": "checklist",
    }

    for keyword, tag in keyword_tags.items():
        if keyword in combined and tag not in tags:
            tags.append(tag)

    return tags or ["prompt-engineering"]


def assign_module(tags: list) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥—É–ª—å –∫—É—Ä—Å–∞ –ø–æ —Ç–µ–≥–∞–º."""
    for tag in tags:
        if tag in TAG_MODULE_MAP:
            return TAG_MODULE_MAP[tag]
    return "01-foundations"


def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP) -> list:
    """–†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è RAG."""
    # –ü—Ä–æ—Å—Ç–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        chunk_words = words[i:i + chunk_size]
        chunks.append(" ".join(chunk_words))
        i += chunk_size - overlap
    return chunks


def load_index() -> dict:
    if INDEX_FILE.exists():
        return json.loads(INDEX_FILE.read_text())
    return {"total_articles": 0, "total_chunks": 0, "last_updated": "", "article_ids": []}


def save_index(index: dict):
    index["last_updated"] = datetime.now().strftime("%Y-%m-%d")
    INDEX_FILE.write_text(json.dumps(index, ensure_ascii=False, indent=2))


def process():
    log("=" * 60)
    log("üîß –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞")

    PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
    CHUNKS_DIR.mkdir(parents=True, exist_ok=True)

    index = load_index()
    raw_files = sorted(RAW_DIR.glob("*.md"))
    log(f"üìÇ –ù–∞–π–¥–µ–Ω–æ raw —Ñ–∞–π–ª–æ–≤: {len(raw_files)}")

    new_articles = 0
    new_chunks = 0

    for raw_path in raw_files:
        try:
            # –ü–∞—Ä—Å–∏–º raw
            data = parse_raw_md(raw_path)
            content = clean_text(data.get("content", ""))

            if len(content) < 300:
                log(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫ (–º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞): {raw_path.name}")
                continue

            # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ hash
            art_id = content_hash(content)
            if art_id in index.get("article_ids", []):
                continue

            # –¢–µ–≥–∏ –∏ –º–æ–¥—É–ª—å
            tags = extract_tags(content, data.get("title", ""))
            module = assign_module(tags)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º processed JSON
            processed = {
                "id": art_id,
                "title": data.get("title", ""),
                "source": "medium",
                "url": data.get("url", ""),
                "author": data.get("author", ""),
                "published": data.get("published", ""),
                "scraped": data.get("scraped", ""),
                "tags": tags,
                "module": module,
                "word_count": len(content.split()),
                "content": content,
            }

            proc_path = PROCESSED_DIR / f"{art_id}.json"
            proc_path.write_text(json.dumps(processed, ensure_ascii=False, indent=2))

            # –ß–∞–Ω–∫–∏–Ω–≥
            chunks = chunk_text(content)
            chunks_data = []
            for i, chunk in enumerate(chunks):
                chunk_obj = {
                    "article_id": art_id,
                    "article_title": data.get("title", ""),
                    "article_url": data.get("url", ""),
                    "module": module,
                    "tags": tags,
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "text": chunk,
                }
                chunks_data.append(chunk_obj)

            chunks_path = CHUNKS_DIR / f"{art_id}.json"
            chunks_path.write_text(json.dumps(chunks_data, ensure_ascii=False, indent=2))

            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å
            index.setdefault("article_ids", []).append(art_id)
            new_articles += 1
            new_chunks += len(chunks)
            log(f"‚úÖ [{new_articles}] '{data['title'][:50]}' ‚Üí {module} ({len(chunks)} —á–∞–Ω–∫–æ–≤)")

        except Exception as e:
            log(f"‚ùå {raw_path.name}: {e}")

    index["total_articles"] = len(index.get("article_ids", []))
    index["total_chunks"] = index.get("total_chunks", 0) + new_chunks
    save_index(index)

    log(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ: +{new_articles} —Å—Ç–∞—Ç–µ–π, +{new_chunks} —á–∞–Ω–∫–æ–≤")
    log(f"üìä –í—Å–µ–≥–æ: {index['total_articles']} —Å—Ç–∞—Ç–µ–π, {index['total_chunks']} —á–∞–Ω–∫–æ–≤")


if __name__ == "__main__":
    process()
