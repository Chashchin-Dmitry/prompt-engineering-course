[
  {
    "article_id": "b4d9754fcb5bc589",
    "article_title": "Tracking LLM Hallucinations Using ReAct-Style Reasoning (Without Telling the Model the Rules)",
    "article_url": "https://medium.com/@singh.tarus/tracking-llm-hallucinations-using-react-style-reasoning-without-telling-the-model-the-rules-855ca6147c29",
    "module": "02-core-techniques",
    "tags": [
      "chain-of-thought",
      "agent"
    ],
    "chunk_index": 0,
    "total_chunks": 4,
    "text": "Tracking LLM Hallucinations Using ReAct-Style Reasoning (Without Telling the Model the Rules) Tarush Singh 6 min read · Dec 16, 2025 -- Press enter or click to view image in full size Most evaluations of Large Language Models (LLMs) focus on a single question: Did the model produce the correct final answer? This approach is simple, but it hides a critical failure mode: LLMs can give correct answers for the wrong reasons. In real systems — analytics tools, financial workflows, internal decision engines, or autonomous agents — this is dangerous. A model that “gets lucky” today may fail catastrophically tomorrow. This article presents a simple and interpretable method to track hallucinations by evaluating how a model reasons, not just what it outputs. The Core Idea (In Plain English) We ask the LLM to solve a problem using ReAct-style step-by-step reasoning . We do not tell the model which rule or framework to use . We capture the model’s intermediate reasoning steps. We score those steps on: Usefulness Correctness Risk We store and analyse these steps to detect hallucination patterns. This lets us answer a better question: “Did the model actually understand the problem — or did it just get lucky?” A Simple Example (Math, Not Theory) Why We Intentionally Do Not Give the Rules Consider a simple math expression: Question: 8 + 2 × (3 + 4) If we tell the model: “Solve this using PEMDAS” We are testing whether it can follow instructions . If we don’t tell it that, we test something more important: Can the model identify which reasoning framework applies? This distinction matters in real-world tasks where: Rules are implicit Context is ambiguous The model must choose how to reason before executing Hallucinations often occur at this rule-selection step , not at calculation. Why Start with Math? Math provides: Deterministic ground truth Clear step ordering Easy verification This isolates reasoning quality from domain ambiguity. The same method generalises to SQL generation, analytics, finance, and agent planning. Using ReAct to Capture Reasoning We use a ReAct-style format that separates reasoning into explicit steps: Thought — what the model believes or plans Action — the operation it performs Observation — the result of that action Final Answer — the output Example (simplified): Thought: Parentheses indicate grouped operations. Action: Compute (3 + 4). Observation: 7 Thought: Multiplication should be resolved next. Action: Compute 2 × 7. Observation: 14 Thought: Add remaining values. Action: Compute 8 + 14. Observation: 22 Final Answer: 22 Each Thought is treated as data , not disposable text. Why We Store Reasoning Separately Instead of discarding the chain of thought, we store it so we can: Compare reasoning across many runs Identify recurring hallucination patterns Detect risky reasoning early The final answer becomes only one signal — The reasoning path becomes the primary one. Turning Reasoning into Measurable Metrics To make hallucination tracking concrete, each reasoning step is evaluated using three simple metrics . These are designed to be intuitive and interpretable, not academic. 1."
  },
  {
    "article_id": "b4d9754fcb5bc589",
    "article_title": "Tracking LLM Hallucinations Using ReAct-Style Reasoning (Without Telling the Model the Rules)",
    "article_url": "https://medium.com/@singh.tarus/tracking-llm-hallucinations-using-react-style-reasoning-without-telling-the-model-the-rules-855ca6147c29",
    "module": "02-core-techniques",
    "tags": [
      "chain-of-thought",
      "agent"
    ],
    "chunk_index": 1,
    "total_chunks": 4,
    "text": "Detect risky reasoning early The final answer becomes only one signal — The reasoning path becomes the primary one. Turning Reasoning into Measurable Metrics To make hallucination tracking concrete, each reasoning step is evaluated using three simple metrics . These are designed to be intuitive and interpretable, not academic. 1. Usefulness Question: Does this step meaningfully help solve the problem? Why it matters: LLMs often generate reasoning that sounds intelligent but does not move the solution forward. Examples: Reasoning Step Usefulness: “Parentheses group operations, so I’ll evaluate them first.” -> High “There are several numbers in this expression.” -> Low Usefulness filters out: Filler reasoning Circular explanations Token-heavy but empty steps 2. Correctness Question: Is the reasoning logically or factually correct? Why it matters: A model can reach the correct final answer while relying on incorrect internal reasoning . Examples: Reasoning Step Correctness “Parentheses are evaluated before multiplication.” -> Correct “I’ll add numbers from left to right.” -> Incorrect This metric exposes hidden hallucinations that accuracy metrics miss. 3. Risk Question: How likely is this reasoning to fail on slightly different inputs? Why it matters: Some reasoning works only in simple cases and collapses in real-world scenarios. High-risk signals include: Guessing Overconfidence without explanation “This usually works” logic Rules applied without justification Examples: Reasoning Step Risk: “Parentheses explicitly group operations.” -> Low “This should be fine.” -> High Risk identifies answers that are accidentally correct . How These Metrics Work Together Each metric captures a different failure mode: Usefulness → Is the reasoning doing real work? Correctness → Is it actually right? Risk → Will it generalise? Hallucinations often appear as: Correct final answer Incorrect or risky reasoning Confident language These metrics make that mismatch visible. A Simple Prompt to Enable This Method Below is a minimal prompt that works with any LLM. Notice what’s missing: no mention of PEMDAS or operator precedence . You are solving a problem step by step. Follow this format strictly: Thought: Describe what you are thinking. Action: Perform a single operation. Observation: Result of that operation. Repeat until you reach a conclusion. Do not skip steps. Do not jump to the final answer. Question: {QUESTION} This prompt exposes the model’s natural reasoning behaviour . Minimal Python Example This example: Sends a math question Captures the reasoning trace Produces raw data for evaluation from openai import OpenAI client = OpenAI() PROMPT_TEMPLATE = \"\"\" You are solving a problem step by step. Follow this format strictly: Thought: Describe what you are thinking. Action: Perform a single operation. Observation: Result of that operation. Repeat until you reach a conclusion. Do not skip steps. Do not jump to the final answer. Question: {question} \"\"\" def run_reasoning(question): prompt = PROMPT_TEMPLATE.format(question=question) response = client.chat.completions.create( model=\"gpt-4o-mini\", messages=[ {\"role\": \"user\", \"content\": prompt} ], temperature=0 ) return response.choices[0].message.content if __name__ == \"__main__\": question = \"8 + 2 * (3 + 4)\" reasoning_trace = run_reasoning(question) print(reasoning_trace) This output becomes your reasoning dataset . Each Thought can now be scored on usefulness, correctness, and risk."
  },
  {
    "article_id": "b4d9754fcb5bc589",
    "article_title": "Tracking LLM Hallucinations Using ReAct-Style Reasoning (Without Telling the Model the Rules)",
    "article_url": "https://medium.com/@singh.tarus/tracking-llm-hallucinations-using-react-style-reasoning-without-telling-the-model-the-rules-855ca6147c29",
    "module": "02-core-techniques",
    "tags": [
      "chain-of-thought",
      "agent"
    ],
    "chunk_index": 2,
    "total_chunks": 4,
    "text": "PROMPT_TEMPLATE.format(question=question) response = client.chat.completions.create( model=\"gpt-4o-mini\", messages=[ {\"role\": \"user\", \"content\": prompt} ], temperature=0 ) return response.choices[0].message.content if __name__ == \"__main__\": question = \"8 + 2 * (3 + 4)\" reasoning_trace = run_reasoning(question) print(reasoning_trace) This output becomes your reasoning dataset . Each Thought can now be scored on usefulness, correctness, and risk. Introducing a Scoring Agent To avoid manual evaluation, we introduce a Scoring Agent — a lightweight LLM prompt whose only job is to judge reasoning , not generate it. The scoring agent receives: The original question A single reasoning step The step’s position in the reasoning chain It outputs structured scores , not prose. Scoring Agent Prompt (Conceptual) You are evaluating a reasoning step produced by another AI. Question: {question} Reasoning Step: {thought} Evaluate this step on a scale of 0 to 1 for: - Usefulness: Does this step help solve the problem? - Correctness: Is the reasoning logically correct? - Risk: How likely is this step to cause errors later? Return ONLY valid JSON: { \"usefulness\": number, \"correctness\": number, \"risk\": number } This agent never sees the final answer. It judges reasoning quality in isolation , which is critical for hallucination detection. Simple Scoring Agent Code Example def score_reasoning_step(question, thought, client): prompt = f\"\"\" You are evaluating a reasoning step produced by another AI.Question: {question} Reasoning Step: {thought} Evaluate this step on a scale of 0 to 1 for: - usefulness - correctness - risk Return ONLY valid JSON. \"\"\" response = client.chat.completions.create( model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0 ) return response.choices[0].message.content Each reasoning trace now becomes a list of scored steps . Metric Aggregation: Turning Scores into Hallucination Signals Once steps are scored, we aggregate them to evaluate the entire reasoning chain . 1. Average Scores (Baseline) avg_usefulness avg_correctness avg_risk This gives a high-level quality snapshot. 2. Early-Error Detection Hallucinations often start early . We track: First incorrect step index First high-risk step index Early failures are far more dangerous than late ones. 3. Risk-Weighted Correctness A useful composite score: trust_score = avg_correctness × (1 − avg_risk) This penalises answers that are: Correct but supported by risky reasoning 4. Hallucination Flags We flag a reasoning chain if: Final answer is correct but One or more steps have low correctness or Average risk exceeds a threshold This catches accidental correctness . Why Aggregation Matters Without aggregation, reasoning analysis is anecdotal. With aggregation, we can: Compare models Track improvement over time Identify unstable reasoning styles Decide when human review is required This turns reasoning into operational telemetry . Final Takeaway Hallucinations do not appear out of nowhere. They show up first in the reasoning. By: Capturing chain-of-thought, Withholding explicit rules, And scoring reasoning quality with clear metrics We can track hallucinations instead of guessing where they happened . This shifts LLM evaluation from: “Did it work?” to: “Can we trust how it worked?” And that’s the difference between a clever demo and a system you can actually rely on. ---"
  },
  {
    "article_id": "b4d9754fcb5bc589",
    "article_title": "Tracking LLM Hallucinations Using ReAct-Style Reasoning (Without Telling the Model the Rules)",
    "article_url": "https://medium.com/@singh.tarus/tracking-llm-hallucinations-using-react-style-reasoning-without-telling-the-model-the-rules-855ca6147c29",
    "module": "02-core-techniques",
    "tags": [
      "chain-of-thought",
      "agent"
    ],
    "chunk_index": 3,
    "total_chunks": 4,
    "text": "hallucinations instead of guessing where they happened . This shifts LLM evaluation from: “Did it work?” to: “Can we trust how it worked?” And that’s the difference between a clever demo and a system you can actually rely on. ---"
  }
]