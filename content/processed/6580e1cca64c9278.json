{
  "id": "6580e1cca64c9278",
  "title": "E26 : ReAct — Synergizing Reasoning and Acting in Language Models",
  "source": "medium",
  "url": "https://medium.com/papers-i-found/e26-react-synergizing-reasoning-and-acting-in-language-models-14a08f46c33d",
  "author": "Praveen Thenraj",
  "published": "None",
  "scraped": "2026-02-20",
  "tags": [
    "RAG",
    "fine-tuning"
  ],
  "module": "03-advanced",
  "word_count": 1310,
  "content": "Press enter or click to view image in full size\nPhoto by\nAbsolutVision\non\nUnsplash\nE26 : ReAct — Synergizing Reasoning and Acting in Language Models\nPraveen Thenraj\n6 min read\n·\nJun 23, 2024\n--\nCombining reasoning along with actioning helps to solve knowledge-intensive reasoning and interactive decision making problems better than using them as separate strategies\nPaper Name :\nReAct - Synergizing Reasoning and Acting in Language Models\nPaper URL :\nhttps://arxiv.org/pdf/2210.03629\nAuthors :\nPrinceton University - Shunyu Yao, Karthik Narasimhan\nGoogle Research, Brain team - Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Cao\nConference :\nICLR 2023\nPlease find annotated paper\nhere\n.\nProblem Statement :\nExisting reasoning based prompting solutions leverage only the internal knowledge of the models to generate reasoning steps to solve the problems.\nExisting action based prompting solutions execute an action by leveraging only the external knowledge to identify the responses to the problems and does not have any reasoning steps involved.\nFine-tuning or Reinforcement learning based solutions require lot of manpower to create annotated datasets that contain reasoning trajectories.\nSolution :\nA prompt only solution that combines the reasoning power of the model that utilises its internal pre-trained knowledge to augment the actioning power of the models to execute a task to extract proper information from external knowledge source as an observation.\nIn ReAct prompting, the LLM is prompted to create a chain that contains thought-action-observation to solve complex problems\nGiven a question, thought helps to create useful information by reasoning using the model’s internal knowledge. A thought can can be like decomposing a goal into multiple subgoals, creating a plan for the goal, track the completion of subgoals, extracting information from the observation step, handle exceptions and adjust the action plans\nFor a given thought step, the entire previous trajectory of thought-action-observation acts as a context.This helps the ReAct framework to adjust the thoughts accordingly based on the overall goal to accomplish and the subgoals achieved till that point\nThe action step performs a particular action based on the input received from the thought step. In this paper, the action step is designed to perform either of three actions - Search, Lookup and Finish by connecting with external data source\nSearch\n- search[entity] helps to search the entity identified by thought, in wiki page. If found it retrieves the first five sentences from that entity wiki page\nLookup\n- lookup[string] helps to retrieve the next sentence from a wiki page containing that string. This action is similar to Ctrl+F\nFinish\n- finish[answer] helps to finish the current task with answer\nThe result of the action step is captured in observation step which contains the responses to the action initiated.\nExperimentation :\nLLM - PaLM-2 - 540B\nExternal knowledge base - wikidata\nDataset evaluated :\nHotPotQA - multi-hop QA\nFact Verification (Fever) - dataset with claims and labels as SUPPORTS, REFUTES, NOT ENOUGH INFO\nALFWorld - a synthetic text-based game\nWebShop - online shopping environment with human instructions and products\nBaselines evaluated - Standard prompting, CoT, CoT-SC, Act only, ReAct + CoT-Sc, CoT-SC + ReAct, Imitation Learning (IL), Imitation Learning + Reinforcement Learning (IL + RL)\nMetrics used :\nHotpotQA - Exact Match (EM)\nFever - Accuracy\nALFWorld -\nWebShop - Score, SR\nObservations :\nReAct outperforms Act-only prompting on HotpotQA and Fever datasets.\nCoT and CoT-SC outperform ReAct on HotpotQA but falls behind ReAct on Fever dataset\nPerformances of baseline vs ReAct on HotpotQA and Fever datasets\nOn further investigation of taking 50 samples each from success and failure class of HotpotQA through ReAct (100 samples) and CoT (100 samples) and trying to categorise them, it was found that CoT is more prone to hallucination due to its dependency on only internal pre-trained knowledge of the model, whereas ReAct leverages access to external knowledge to avoid hallucination.\nAlso it was found that the rigid structure of ReAct like thought, action, observation sometime causes the model to generate the same thought and action from previous steps again and again which the authors classify as a reasoning error.\nPress enter or click to view image in full size\nCategorising the success and failure scenarios of ReAct and CoT on Hotpot QA\nAlso, the non-informative retrieval such as empty result or non useful search results from an action step in ReAct also contributes to the major failure in ReAct. No or non useful information confuses the model’s reasoning abilities and hampers its ability to formulate and execute the thoughts properly\nReAct -> CoT-SC and CoT-SC -> ReAct strategies improved the performance compared to both CoT and CoT-SC on HotPotQA. When combining the two strategies, limits were set to determine when to switch from ReAct to CoT-SC or vice versa.\nIn ReAct -> CoT-SC, 5 and 7 ReAct steps respectively for HotPotQA and Fever datasets were set as limit before switching to CoT-SC as post these steps, ReAct starts to generate duplicate steps.\nSimilarly, if a majority response does not get more than (n/2) votes, where n is the number of responses generated by CoT-SC then the approach would switch from CoT-SC to ReAct.\nCoT-SC when used solely for HotpotQA, uses 21 samples to obtain the maximum performance, whereas ReAct+CoT-SC achieves this with 3–5 samples for voting.\nNumber of samples in CoT-SC vs ReAct -> CoT-SC vs CoT-SC -> ReAct\nPerformance of ReAct using smaller models like PaLM-2 (8B) was less compared to bigger models like PaLM-2 (64B) and PaLM-2 (540B) on HotpotQA which is an expected behaviour due to the better reasoning and action planning and executing capabilities of the models with increased parameter size\nPrompting Vs Fine-tuning and Prompting\nHowever when fine-tuned on a dataset containing 3000 trajectories (thoughts-actions-observations)and then evaluated, ReAct based prompting outperforms standard, CoT and Act prompting methods even when using smaller model (PaLM2–8B).\nAlso it can be seen that, ReAct with fine-tuning for 8B model outperforms 62B model using prompting only techniques. Similarly, ReAct with fine-tuning for 64B model outperforms prompting only techniques used with 540B model\nReAct outperforms Act-only comfortably on decision making tasks like AlfWorld. One possible reason attributed is the reasoning step involved in ReAct which helps to use the commonsense knowledge of the model to solve AlfWorld tasks that contain household related tasks.\nPress enter or click to view image in full size\nPerformance on AlfWorld dataset\nReAct even outperforms BUTLER - a method where the imitation agents were trained on huge corpus of such tasks\nReAct also outperforms Act-only on Webshop dataset where given a user instruction, the objective is to identify the product to buy based on all the attributes mentioned by the user. Here score is used to identify the number of times correct attributes is identified out of the total samples and Success Rate(SR) is used to identify how many times all the attributes of a sample are correctly identified out of the total samples taken for validation\nComparision of ReAct with baselines for WebShop dataset\nReAct also outperforms imitation learning and imitation learning with reinforcement learning techniques on WebShop dataset.\nReAct when tested with GPT-3 model instead of PaLM-2 model maintains robustness in performance thus proving that the approach can work with different models. Infact, GPT-3 outperformed PaLM-2 model performance on HotpotQA task which may be attributed to the instruction fine-tuning done for GPT-3.\nConclusion :\nReAct mimics a human way of using the reasoning abilities to plan and execute a task which helps it to perform better compared to other strategies.\nReAct when succeeded or preceeded by CoT-SC shows even more promising results which shows that some tasks would pre-dominantly require reasoning based on internal knowledge of model (like in CoT) whereas some tasks would require action based on external knowledge (like in Act-only)\nResults also prove that combining ReAct with other techniques like fine-tuning, reinforcement learning can further improve the performance of ReAct based prompting.\n\n---",
  "summary": "Разбор оригинальной научной статьи ReAct (Google Research). ReAct = Reasoning + Acting: модель чередует Thought (рассуждение), Action (действие/поиск), Observation (результат). Показывает почему это лучше чем просто CoT или просто поиск.",
  "summary_en": "Breakdown of the original ReAct paper (Google Research). ReAct = Reasoning + Acting: model alternates Thought (reasoning), Action (search/API call), Observation (result). Shows why this beats pure CoT or pure search.",
  "key_insights": [
    "ReAct = Chain of Thought + Tool Use в одном промте",
    "Thought → Action → Observation цикл повторяется до получения ответа",
    "Лучше CoT: добавляет актуальные данные из внешних источников",
    "Лучше чистого поиска: добавляет интерпретацию и рассуждение",
    "Применимо к Wikipedia API, SQL, веб-поиску — любому инструменту",
    "Ключевой инсайт: языковая модель как оркестратор планирования И исполнения"
  ],
  "practical_example": "Question: 'Кто снял фильм X и сколько ему лет?' → Thought: нужна дата рождения → Action: Wikipedia search → Observation: [данные] → Thought: посчитаю возраст → Final Answer",
  "course_relevance": "Фундаментальный материал для модуля 03 (продвинутые техники). Академически строгий, хорошая теоретическая база.",
  "quality_score": 9,
  "difficulty": "intermediate"
}