{
  "id": "b4d9754fcb5bc589",
  "title": "Tracking LLM Hallucinations Using ReAct-Style Reasoning (Without Telling the Model the Rules)",
  "source": "medium",
  "url": "https://medium.com/@singh.tarus/tracking-llm-hallucinations-using-react-style-reasoning-without-telling-the-model-the-rules-855ca6147c29",
  "author": "Tarush Singh",
  "published": "None",
  "scraped": "2026-02-20",
  "tags": [
    "chain-of-thought",
    "agent"
  ],
  "module": "02-core-techniques",
  "word_count": 1390,
  "content": "Tracking LLM Hallucinations Using ReAct-Style Reasoning (Without Telling the Model the Rules)\nTarush Singh\n6 min read\n·\nDec 16, 2025\n--\nPress enter or click to view image in full size\nMost evaluations of Large Language Models (LLMs) focus on a single question:\nDid the model produce the correct final answer?\nThis approach is simple, but it hides a critical failure mode:\nLLMs can give correct answers for the wrong reasons.\nIn real systems — analytics tools, financial workflows, internal decision engines, or autonomous agents — this is dangerous. A model that “gets lucky” today may fail catastrophically tomorrow.\nThis article presents a\nsimple and interpretable method to track hallucinations\nby evaluating\nhow\na model reasons, not just\nwhat\nit outputs.\nThe Core Idea (In Plain English)\nWe ask the LLM to solve a problem using\nReAct-style step-by-step reasoning\n.\nWe\ndo not tell the model which rule or framework to use\n.\nWe capture the model’s intermediate reasoning steps.\nWe score those steps on:\nUsefulness\nCorrectness\nRisk\nWe store and analyse these steps to detect hallucination patterns.\nThis lets us answer a better question:\n“Did the model actually understand the problem — or did it just get lucky?”\nA Simple Example (Math, Not Theory)\nWhy We Intentionally Do Not Give the Rules\nConsider a simple math expression:\nQuestion:\n8 + 2 × (3 + 4)\nIf we tell the model:\n“Solve this using PEMDAS”\nWe are testing whether it can\nfollow instructions\n.\nIf we\ndon’t\ntell it that, we test something more important:\nCan the model identify which reasoning framework applies?\nThis distinction matters in real-world tasks where:\nRules are implicit\nContext is ambiguous\nThe model must choose\nhow\nto reason before executing\nHallucinations often occur at this\nrule-selection step\n, not at calculation.\nWhy Start with Math?\nMath provides:\nDeterministic ground truth\nClear step ordering\nEasy verification\nThis isolates\nreasoning quality\nfrom domain ambiguity.\nThe same method generalises to SQL generation, analytics, finance, and agent planning.\nUsing ReAct to Capture Reasoning\nWe use a ReAct-style format that separates reasoning into explicit steps:\nThought\n— what the model believes or plans\nAction\n— the operation it performs\nObservation\n— the result of that action\nFinal Answer\n— the output\nExample (simplified):\nThought: Parentheses indicate grouped operations.\nAction: Compute (3 + 4).\nObservation: 7\nThought: Multiplication should be resolved next.\nAction: Compute 2 × 7.\nObservation: 14\nThought: Add remaining values.\nAction: Compute 8 + 14.\nObservation: 22\nFinal Answer: 22\nEach\nThought\nis treated as\ndata\n, not disposable text.\nWhy We Store Reasoning Separately\nInstead of discarding the chain of thought, we store it so we can:\nCompare reasoning across many runs\nIdentify recurring hallucination patterns\nDetect risky reasoning early\nThe final answer becomes only one signal —\nThe reasoning path becomes the primary one.\nTurning Reasoning into Measurable Metrics\nTo make hallucination tracking concrete, each reasoning step is evaluated using\nthree simple metrics\n. These are designed to be intuitive and interpretable, not academic.\n1. Usefulness\nQuestion:\nDoes this step meaningfully help solve the problem?\nWhy it matters:\nLLMs often generate reasoning that sounds intelligent but does not move the solution forward.\nExamples:\nReasoning Step Usefulness:\n“Parentheses group operations, so I’ll evaluate them first.” -> High\n“There are several numbers in this expression.” -> Low\nUsefulness filters out:\nFiller reasoning\nCircular explanations\nToken-heavy but empty steps\n2. Correctness\nQuestion:\nIs the reasoning logically or factually correct?\nWhy it matters:\nA model can reach the correct final answer while relying on\nincorrect internal reasoning\n.\nExamples:\nReasoning Step Correctness\n“Parentheses are evaluated before multiplication.” -> Correct\n“I’ll add numbers from left to right.” -> Incorrect\nThis metric exposes\nhidden hallucinations\nthat accuracy metrics miss.\n3. Risk\nQuestion:\nHow likely is this reasoning to fail on slightly different inputs?\nWhy it matters:\nSome reasoning works only in simple cases and collapses in real-world scenarios.\nHigh-risk signals include:\nGuessing\nOverconfidence without explanation\n“This usually works” logic\nRules applied without justification\nExamples:\nReasoning Step Risk:\n“Parentheses explicitly group operations.” -> Low\n“This should be fine.” -> High\nRisk identifies\nanswers that are accidentally correct\n.\nHow These Metrics Work Together\nEach metric captures a different failure mode:\nUsefulness\n→ Is the reasoning doing real work?\nCorrectness\n→ Is it actually right?\nRisk\n→ Will it generalise?\nHallucinations often appear as:\nCorrect final answer\nIncorrect or risky reasoning\nConfident language\nThese metrics make that mismatch visible.\nA Simple Prompt to Enable This Method\nBelow is a minimal prompt that works with any LLM.\nNotice what’s missing:\nno mention of PEMDAS or operator precedence\n.\nYou are solving a problem step by step.\nFollow this format strictly:\nThought: Describe what you are thinking.\nAction: Perform a single operation.\nObservation: Result of that operation.\nRepeat until you reach a conclusion.\nDo not skip steps.\nDo not jump to the final answer.\nQuestion:\n{QUESTION}\nThis prompt exposes the model’s\nnatural reasoning behaviour\n.\nMinimal Python Example\nThis example:\nSends a math question\nCaptures the reasoning trace\nProduces raw data for evaluation\nfrom openai import OpenAI\nclient = OpenAI()\nPROMPT_TEMPLATE = \"\"\"\nYou are solving a problem step by step.\nFollow this format strictly:\nThought: Describe what you are thinking.\nAction: Perform a single operation.\nObservation: Result of that operation.\nRepeat until you reach a conclusion.\nDo not skip steps.\nDo not jump to the final answer.\nQuestion:\n{question}\n\"\"\"\ndef run_reasoning(question):\nprompt = PROMPT_TEMPLATE.format(question=question)\nresponse = client.chat.completions.create(\nmodel=\"gpt-4o-mini\",\nmessages=[\n{\"role\": \"user\", \"content\": prompt}\n],\ntemperature=0\n)\nreturn response.choices[0].message.content\nif __name__ == \"__main__\":\nquestion = \"8 + 2 * (3 + 4)\"\nreasoning_trace = run_reasoning(question)\nprint(reasoning_trace)\nThis output becomes your\nreasoning dataset\n.\nEach\nThought\ncan now be scored on usefulness, correctness, and risk.\nIntroducing a Scoring Agent\nTo avoid manual evaluation, we introduce a\nScoring Agent\n— a lightweight LLM prompt whose only job is to\njudge reasoning\n, not generate it.\nThe scoring agent receives:\nThe original question\nA single reasoning step\nThe step’s position in the reasoning chain\nIt outputs\nstructured scores\n, not prose.\nScoring Agent Prompt (Conceptual)\nYou are evaluating a reasoning step produced by another AI.\nQuestion:\n{question}\nReasoning Step:\n{thought}\nEvaluate this step on a scale of 0 to 1 for:\n- Usefulness: Does this step help solve the problem?\n- Correctness: Is the reasoning logically correct?\n- Risk: How likely is this step to cause errors later?\nReturn ONLY valid JSON:\n{\n\"usefulness\": number,\n\"correctness\": number,\n\"risk\": number\n}\nThis agent never sees the final answer.\nIt judges\nreasoning quality in isolation\n, which is critical for hallucination detection.\nSimple Scoring Agent Code Example\ndef score_reasoning_step(question, thought, client):\nprompt = f\"\"\"\nYou are evaluating a reasoning step produced by another AI.Question:\n{question}\nReasoning Step:\n{thought}\nEvaluate this step on a scale of 0 to 1 for:\n- usefulness\n- correctness\n- risk\nReturn ONLY valid JSON.\n\"\"\"\nresponse = client.chat.completions.create(\nmodel=\"gpt-4o-mini\",\nmessages=[{\"role\": \"user\", \"content\": prompt}],\ntemperature=0\n)\nreturn response.choices[0].message.content\nEach reasoning trace now becomes a\nlist of scored steps\n.\nMetric Aggregation: Turning Scores into Hallucination Signals\nOnce steps are scored, we aggregate them to evaluate the\nentire reasoning chain\n.\n1. Average Scores (Baseline)\navg_usefulness\navg_correctness\navg_risk\nThis gives a high-level quality snapshot.\n2. Early-Error Detection\nHallucinations often start\nearly\n.\nWe track:\nFirst incorrect step index\nFirst high-risk step index\nEarly failures are far more dangerous than late ones.\n3. Risk-Weighted Correctness\nA useful composite score:\ntrust_score = avg_correctness × (1 − avg_risk)\nThis penalises answers that are:\nCorrect but supported by risky reasoning\n4. Hallucination Flags\nWe flag a reasoning chain if:\nFinal answer is correct\nbut\nOne or more steps have low correctness\nor\nAverage risk exceeds a threshold\nThis catches\naccidental correctness\n.\nWhy Aggregation Matters\nWithout aggregation, reasoning analysis is anecdotal.\nWith aggregation, we can:\nCompare models\nTrack improvement over time\nIdentify unstable reasoning styles\nDecide when human review is required\nThis turns reasoning into\noperational telemetry\n.\nFinal Takeaway\nHallucinations do not appear out of nowhere.\nThey show up first in the reasoning.\nBy:\nCapturing chain-of-thought,\nWithholding explicit rules,\nAnd scoring reasoning quality with clear metrics\nWe can\ntrack hallucinations instead of guessing where they happened\n.\nThis shifts LLM evaluation from:\n“Did it work?”\nto:\n“Can we trust how it worked?”\nAnd that’s the difference between a clever demo and a system you can actually rely on.\n\n---",
  "summary": "Продвинутый метод обнаружения галлюцинаций через анализ ПРОЦЕССА рассуждения, а не только финального ответа. Вводит 3 метрики: Usefulness, Correctness, Risk для каждого шага цепочки мыслей. Модель может дать правильный ответ по неправильной причине — именно это опасно в продакшене.",
  "summary_en": "Advanced method for detecting LLM hallucinations by analyzing the reasoning PROCESS, not just the final answer. Introduces 3 metrics per reasoning step: Usefulness, Correctness, Risk. A model can give a correct answer for the wrong reasons — that's the real danger in production.",
  "key_insights": [
    "Правильный ответ ≠ правильное рассуждение. Модель может 'угадать' верно, но это ненадёжно",
    "Не говори модели какие правила использовать — это тест понимания, не следования инструкциям",
    "3 метрики: Usefulness (шаг помогает?), Correctness (шаг верен?), Risk (обобщаемо ли?)",
    "trust_score = avg_correctness × (1 − avg_risk) — формула надёжности",
    "Галлюцинации чаще появляются РАНО в цепочке рассуждений, не в конце",
    "Отдельный scoring agent оценивает шаги независимо — не видит финальный ответ"
  ],
  "practical_example": "Python код с OpenAI API: промт заставляет Thought/Action/Observation, затем scoring agent оценивает каждый шаг по 3 метрикам",
  "course_relevance": "Продвинутый материал для модуля 03. Полезно для команд строящих production AI-системы.",
  "quality_score": 9,
  "difficulty": "advanced"
}